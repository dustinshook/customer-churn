---
title: "Telco Customer Churn"
author: "Dustin Shook"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(tidyverse)
library(GGally)

raw_data <- readr::read_csv('data/WA_Fn-UseC_-Telco-Customer-Churn.csv')
```

# Objectives

Explore Telco customer churn with a data set from [Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

-   Data Manipulation: `dplyr`, `tidyr`, and `tibble`
-   EDA & visualizations: `skimr` and `ggplot`
-   Customer Segmentation with `kmeans()` and `umap()`
-   Modeling: `tidymodels` framework.

# Explore

```{r General EDA}
skimr::skim(raw_data)
```

Over all our data set is pretty clean. It also appears we wont have to deal with any class imbalance issues with our target variable `Churn`.

```{r}
raw_data %>% 
    count(Churn)
```
Let's now turn our focus to the numeric predictors. We can use the package `GGally` to explore relationships in our data. 

I notice right away that our `SeniorCitizen` column is being classified as a numeric value instead of a factor, so let's remove it from our data before plotting our numeric predictors in the plot. We'll add a feature engineering step later to correct this.

```{r Numeric Predictor EDA, message=FALSE, warning=FALSE}
raw_data %>% 
    select(
        Churn, 
        where(is.numeric), 
        -SeniorCitizen # categorical variable, fixed later
    ) %>% 
    ggpairs(
        aes(color = Churn), 
        legend = 1, 
        diag = list(continuous = wrap('densityDiag', alpha = .5))
    ) +
    theme(legend.position = "bottom")
```

Now that we have a better understanding of our numeric predictors, let's turn our attention to categorical variables.

In this step, I'll add in the `SeniorCitizen` column by quickly encoding it to the same format as our other categorical columns with `case_match` for consistency.

```{r Categorical Predictor EDA, message=FALSE, warning=FALSE}
raw_data %>% 
    mutate(
        SeniorCitizen = case_match(
            SeniorCitizen,
            1 ~ 'Yes',
            0 ~ 'No',
            .default = 'No'
        )
    ) %>% 
    select(Churn, where(is.character), -customerID) %>% 
    pivot_longer(!Churn) %>%
    ggplot(aes(y = value, fill = Churn)) +
    geom_bar(position = "fill") +
    facet_wrap(vars(name), scales = "free", ncol = 2) +
    labs(x = NULL, y = NULL, fill = NULL)
```
I notice that we have a rather redundant factor level `"No internet service"` with several of our categorical predictors. I decide to remove this level from all columns because it is already accounted for by the `InternetService` column. We'll move forward from this point using our new `parsed_data` that contains the feature engineering we've added thus far.

```{r Feature Engineering}
parsed_data <- 
    raw_data %>% 
    mutate(
        across( 
            c(# re code the following categorical predictors
                DeviceProtection, 
                OnlineBackup, 
                StreamingTV, 
                TechSupport, 
                StreamingMovies, 
                OnlineSecurity
                # if the factor level is 'no internet service' replace with a simple 'no'
            ), ~ if_else(.x == 'No internet service', 'No', .x)
        ),
        # do the same for the multi phone lines column for consistency
        MultipleLines = if_else(MultipleLines == 'No phone service', 'No', MultipleLines),
        # re code the Senior Citizen column for consistency
        SeniorCitizen = case_match(
            SeniorCitizen,
            1 ~ 'Yes',
            0 ~ 'No',
            .default = 'No'
        )
    ) %>% 
    na.omit()
```

Let's now take a second look at our categorical predictors visual, this time using `parsed_data` instead of our `raw_data`

```{r}
parsed_data %>% 
    select(Churn, where(is.character), -customerID) %>% 
    pivot_longer(!Churn) %>%
    ggplot(aes(y = value, fill = Churn)) +
    geom_bar(position = "fill") +
    facet_wrap(vars(name), scales = "free", ncol = 2) +
    labs(x = NULL, y = NULL, fill = NULL)
```
# Customer Segmentation

We'll start our segmentation by creating a 2D representation of our 18 predictor variables with `umap()`, but first we need to get our data in the correct format by turning our categorical variables into numeric and scaling our numeric variables. We'll also remove `gender`, `customerID`, and our target variable `Churn`

```{r Umap data preperation}
kmeans_data <-
    parsed_data %>% 
    select(-gender, -Churn) %>%
    mutate(
        across(where(is.numeric), ~ as.numeric(scale(.))),
        across(where(is.character), ~ as.factor(.)),
        customerID = as.character(customerID),
        across(where(is.factor), ~ as.numeric(.))
    ) 
```

Next, with the data properly formatted we'll generate a 2D projection.

```{r message=FALSE, warning=FALSE}
library(umap)

churn_umap <- 
    kmeans_data %>% 
    select(-customerID) %>% 
    umap()

churn_umap_tbl <-
    churn_umap %>% 
    pluck("layout") %>% 
    as_tibble(.name_repair = "unique") %>% 
    set_names("x", "y") %>% 
    bind_cols(
        select(parsed_data, customerID)
    )

churn_umap_tbl %>% 
    ggplot(aes(x,y)) +
    geom_point(alpha = 0.5) +
    labs(
        title = "UMAP Projection"
    ) 
    
```

Now we'll turn our focus to `kmeans()` and choosing the correct value for k (centers). We'll do this by creating a different cluster object for each value of k 2:7

```{r}
cluster_analysis <-
    # create a tibble with values k 2:7
    tibble(k = 2:7) %>% 
    # include k in call to row wise so that k is included in re frames
    rowwise(k) %>% 
    mutate(
        clusters = list(
            kmeans( # remove customer Id column
                select(churn_umap_tbl, -customerID), 
                centers = k
            )
        )
    )

tidy_clust <- cluster_analysis %>% 
    reframe(
        tidy(clusters)
    )

glance_clust <- cluster_analysis %>% 
    reframe(
        glance(clusters)
    )

augment_clust <- cluster_analysis %>% 
    reframe( # augment the results onto our 2D projection instead of kmeans_data
        augment(clusters, churn_umap_tbl)
    )

augment_clust %>% 
    ggplot(aes(x = x, y = y)) +
    geom_point(aes(color = .cluster), alpha = 0.5) +
    facet_wrap(~k) +
    geom_point(data = tidy_clust, size = 10, shape = "x")
```
With the information we have thus far, 3 or 4 seems like the best value for k. Let's try another approach with a skree plot and see if we gain anymore insight. 

```{r}
glance_clust %>%
  ggplot(aes(k, tot.withinss)) +
  geom_line(alpha = 0.5, size = 1.2, color = "blue") +
  geom_point(size = 2, color = "blue") +
    labs(
        title = "Skree Plot",
        subtitle = "Measure the distance from the kmeans center"
    )
    
```

Judging by our skree plot the constant rate of change appears linear after `k = 3`, although one could argue that 4 may be the best solution.

```{r}
churn_clust_final <- kmeans(
    select(churn_umap_tbl, -customerID), 
    centers = 3
)

glance(churn_clust_final)
```

Let's go back to our visualization and see if we can draw anymore insights now with the proper k value.

```{r}
augment(churn_clust_final, parsed_data) %>% 
    select(Churn, .cluster) %>% 
    pivot_longer(!Churn) %>%
    ggplot(aes(y = value, fill = Churn)) +
    geom_bar(position = "fill") +
    facet_wrap(vars(name), scales = "free", ncol = 2) +
    labs(x = NULL, y = NULL, fill = NULL)
```
Not bad, we have created a way to segment our customer data into 3 different categories - low, medium, and highest chance at customer churn. Let's now move on to a modeling approach - we may even be able to utilize our new segments.

# Build our Model

Our first model we'll build is 

```{r}
churn_df <-
    parsed_data %>% 
    select(-gender) %>%
    mutate(
        across(where(is.character), as.factor),
        customerID = as.character(customerID)
    ) 

glimpse(churn_df)
```

```{r}
set.seed(1234)

churn_split <- initial_split(churn_df, strata = Churn)

churn_train <- training(churn_split)
churn_test <- testing(churn_split)

set.seed(567)
churn_folds <- vfold_cv(churn_train, strata = Churn)

churn_folds
```

```{r}
churn_recipe <-
    recipe(Churn ~ ., data = churn_train) %>% 
    update_role(customerID, new_role = 'id') %>% 
    step_dummy(all_nominal_predictors()) %>% 
    step_normalize(all_numeric_predictors()) %>% 
    step_zv(all_predictors())

churn_recipe
```


```{r}
xgb_spec <-
    boost_tree(
        trees = tune(),
        min_n = tune(),
        mtry = tune(),
        learn_rate = 0.01
    ) %>% 
    set_engine('xgboost') %>% 
    set_mode('classification')

xgb_workflow <- workflow(churn_recipe, xgb_spec)
```

```{r}
doParallel::registerDoParallel()

set.seed(891)

library(finetune)

xgb_rs <- tune_race_anova(
    xgb_workflow,
    resamples = churn_folds,
    grid = 15,
    control = control_race(verbose_elim = T)
)

xgb_rs
```

```{r}
collect_metrics(xgb_rs)
```

```{r}
plot_race(xgb_rs)
```

```{r}
xgb_final_fit <- xgb_workflow %>% 
    finalize_workflow(select_best(xgb_rs, "accuracy")) %>% 
    last_fit(churn_split)

xgb_final_fit
```

```{r}
collect_metrics(xgb_final_fit)
```

```{r}
collect_predictions(xgb_final_fit) %>% 
    conf_mat(Churn, .pred_class)
```

```{r}
library(vip)

xgb_final_fit %>% 
    extract_fit_engine() %>% 
    vip() 
```
```{r}
test_set_predictions <- augment(xgb_final_fit) 
```


```{r}
final_fitted <- extract_workflow(xgb_final_fit)
```

```{r}
xgb_final_fit %>%
  collect_predictions() %>%
  roc_curve(Churn, .pred_No) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = "midnightblue") +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )
```

```{r}
readr::write_rds(final_fitted, 'models/xgboost_racing_tuned.rds')
```

