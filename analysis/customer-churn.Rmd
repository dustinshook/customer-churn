---
title: "Telco Customer Churn"
author: "Dustin Shook"
date: "`r Sys.Date()`"
output:
    html_document:
        toc: TRUE
        toc_float: TRUE
        theme: flatly
        highlight: tango
        code_folding: hide
        df_print: tibble
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidymodels)
library(tidyverse)
library(GGally)

raw_data <- readr::read_csv('data/WA_Fn-UseC_-Telco-Customer-Churn.csv')
```

# Objectives

Explore Telco customer churn with a data set from [Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

-   Data Manipulation: `dplyr`, `tidyr`, and `tibble`
-   EDA & visualizations: `skimr` and `ggplot`
-   Customer Segmentation with `kmeans()` and `umap()`
-   Modeling: `tidymodels` framework.

# Exploratory Data Analysis

## Step 1 - Initial look

We'll start by using the `skimr` package to get a feel for a data set. Over all this data set is pretty clean and shouldn't require a ton of cleansing.

```{r General EDA}
skimr::skim(raw_data)
```

### Outcome Variable

Our target variable `Churn` is distributed reasonably and we don't appear to have a large class imbalance to deal with.

```{r Target Variable Churn}
raw_data %>% 
    count(Churn)
```

## Step 2 - Numeric Variables

Let's now turn our focus to the numeric predictors. We can use the package `GGally` to explore relationships in our data.

I notice right away that our `SeniorCitizen` column is being classified as a numeric value instead of a factor, so let's remove it from our data before plotting our numeric predictors. We'll add a feature engineering step for this later.

```{r Numeric Predictor EDA, message=FALSE, warning=FALSE}
raw_data %>% 
    select(
        Churn, 
        where(is.numeric), 
        -SeniorCitizen # categorical variable, fixed later
    ) %>% 
    ggpairs(
        aes(color = Churn), 
        legend = 1, 
        diag = list(continuous = wrap('densityDiag', alpha = .5))
    ) +
    theme(legend.position = "bottom") +
    scale_fill_brewer(palette = "Set3")
```

## Step 3 - Categorical Variables

In this step, I'll add in the `SeniorCitizen` column by quickly encoding it to the same format as our other categorical columns with `case_match` for consistency.

```{r Categorical Predictor EDA, fig.height=8, fig.width=8, message=FALSE, warning=FALSE}
raw_data %>% 
    mutate(
        SeniorCitizen = case_match(
            SeniorCitizen,
            1 ~ 'Yes',
            0 ~ 'No',
            .default = 'No'
        )
    ) %>% 
    select(Churn, where(is.character), -customerID) %>% 
    pivot_longer(!Churn) %>%
    ggplot(aes(y = value, fill = Churn)) +
    geom_bar(position = "fill") +
    facet_wrap(vars(name), scales = "free", ncol = 2) +
    labs(
        title = "Telco Customer Churn",
        subtitle = "Categorical Data",
        caption = "Proportion of Customer Churn",
        x = NULL, 
        y = NULL, 
        fill = NULL
    ) +
    scale_fill_brewer(palette = "Set3") +
    theme_dark()
```

### Minor adjustments

I notice that we have a rather redundant factor level `"No internet service"` with several of our categorical predictors. I decide to remove this level from all columns because it is already accounted for by the `InternetService` column. We'll move forward from this point using our new `parsed_data` that contains the feature engineering we've added thus far.

```{r Feature Engineering}
parsed_data <- 
    raw_data %>% 
    mutate(
        across( 
            c(# re code the following categorical predictors
                DeviceProtection, 
                OnlineBackup, 
                StreamingTV, 
                TechSupport, 
                StreamingMovies, 
                OnlineSecurity
                # if the factor level is 'no internet service' replace with a simple 'no'
            ), ~ if_else(.x == 'No internet service', 'No', .x)
        ),
        # do the same for the multi phone lines column for consistency
        MultipleLines = if_else(MultipleLines == 'No phone service', 'No', MultipleLines),
        # re code the Senior Citizen column for consistency
        SeniorCitizen = case_match(
            SeniorCitizen,
            1 ~ 'Yes',
            0 ~ 'No',
            .default = 'No'
        )
    ) %>% 
    na.omit()
```

### Second look

Let's now take a second look at our categorical predictors visual, this time using `parsed_data` instead of our `raw_data`

```{r Categorical Second Look, echo=FALSE, fig.height=8, fig.width=8}
parsed_data %>% 
    select(Churn, where(is.character), -customerID) %>% 
    pivot_longer(!Churn) %>%
    ggplot(aes(y = value, fill = Churn)) +
    geom_bar(position = "fill") +
    facet_wrap(vars(name), scales = "free", ncol = 2) +
    labs(
        title = "Telco Customer Churn",
        subtitle = "Categorical Data",
        caption = "Proportion of Customer Churn",
        x = NULL, 
        y = NULL, 
        fill = NULL
    ) +
    scale_fill_brewer(palette = "Set3") +
    theme_dark() 
```

# Customer Segmentation

## Step 1 - Data preperation

We'll start our segmentation by creating a 2D representation of our 18 predictor variables with `umap()`, but first we need to get our data in the correct format by turning our categorical variables into numeric and scaling our numeric variables. We'll also remove `gender`, `customerID`, and our target variable `Churn`

```{r Umap data preperation}
kmeans_data <-
    parsed_data %>% 
    select(-gender, -Churn) %>%
    mutate(
        across(where(is.numeric), ~ as.numeric(scale(.))),
        across(where(is.character), ~ as.factor(.)),
        customerID = as.character(customerID),
        across(where(is.factor), ~ as.numeric(.))
    ) 
```

## Step 2 - Generate UMAP 2D Projection

Next, with the data properly formatted we can generate a two dimensional representation from our many predictor variables.

```{r message=FALSE, warning=FALSE}
library(umap)

churn_umap <- 
    kmeans_data %>% 
    select(-customerID) %>% 
    umap()

churn_umap_tbl <-
    churn_umap %>% 
    pluck("layout") %>% 
    as_tibble(.name_repair = "unique") %>% 
    set_names("x", "y") %>% 
    bind_cols(
        select(parsed_data, customerID)
    )

churn_umap_tbl %>% 
    ggplot(aes(x,y)) +
    geom_point(alpha = 0.5, color = '#FFFFB3') +
    labs(
        title = "UMAP Projection"
    ) +
    theme_dark()
    
```

## Step 3 - K-Means Clustering

Now we'll turn our focus to `kmeans()` and choosing the correct value for k (centers). We'll do this by creating a different cluster object for each value of k 2:7

```{r}
cluster_analysis <-
    # create a tibble with values k 2:7
    tibble(k = 2:7) %>% 
    # include k in call to row wise so that k is included in re frames
    rowwise(k) %>% 
    mutate(
        clusters = list(
            kmeans( # remove customer Id column
                select(churn_umap_tbl, -customerID), 
                centers = k
            )
        )
    )

tidy_clust <- cluster_analysis %>% 
    reframe(
        tidy(clusters)
    )

glance_clust <- cluster_analysis %>% 
    reframe(
        glance(clusters)
    )

augment_clust <- cluster_analysis %>% 
    reframe(
        augment(clusters, churn_umap_tbl)
    )

augment_clust %>% 
    ggplot(aes(x = x, y = y)) +
    geom_point(aes(color = .cluster), alpha = 0.5) +
    facet_wrap(~k) +
    geom_point(data = tidy_clust, size = 10, shape = "x") +
    scale_color_brewer(palette = "Set3") + 
    theme_dark()
```

### Skree Plot

With the information we have thus far, 3 or 4 seems like the best value for k. Let's try another approach with a Skree plot and see if we gain anymore insight.

```{r}
glance_clust %>%
  ggplot(aes(k, tot.withinss)) +
  geom_line(alpha = 0.6, size = 1.5, color = "#FFFFB3") +
  geom_point(size = 3, color = "#8DD3C7") +
    labs(
        title = "Skree Plot",
        subtitle = "Measure the distance from the kmeans center"
    ) + 
    theme_dark()
    
```

### Determine Best Value for K

Judging by our Skree plot the constant rate of change appears linear after `k = 3`, although one could argue that 4 may be the best solution.

```{r}
churn_clust_final <- kmeans(
    select(churn_umap_tbl, -customerID), 
    centers = 3
)

glance(churn_clust_final)
```

## Step 4 - Visualize Customer Segmentation

Let's go back to our visualization and see if we can draw anymore insights now with the proper k value.

```{r}
augment(churn_clust_final, parsed_data) %>% 
    select(Churn, .cluster) %>% 
    pivot_longer(!Churn) %>%
    ggplot(aes(y = value, fill = Churn)) +
    geom_bar(position = "fill") +
    facet_wrap(vars(name), scales = "free", ncol = 2) +
    labs(
        title = "Telco Customer Segmentation",
        subtitle = "Customer Churn by Cluster",
        caption = "Proportion of Customer Churn",
        x = NULL, 
        y = NULL, 
        fill = NULL
    ) +
    scale_fill_brewer(palette = "Set3") +
    theme_dark() 
```

### Wrapping up

Not bad, we have created a way to segment our customer data into 3 different categories - low, medium, and highest chance at customer churn. Let's now move on to model building.

# Model Building

## Step 1 - Data Prep

### Feature Removal

First things first, we need to prepare our data. I am making the decision to remove the gender variable from our data. It's nearly a perfect split between male/female churn so I don't believe it adds any significance to our objective.

```{r}
parsed_data %>% 
    count(gender, Churn) %>% 
    mutate(prop = n/sum(n))
```

The majority of data preprocessing steps we will handle later using the `recipes` package. For now, we'll simply remove the `gender` column and confirm that characters variables are converted to factors. We are going to use `customerID` as an ID variable instead of removing it, so we need to make sure it remains a character. Let's also take one final `glimpse()` before we move on

```{r}
churn_df <-
    parsed_data %>% 
    select(-gender) %>%
    mutate(
        across(where(is.character), as.factor),
        customerID = as.character(customerID)
    ) 

glimpse(churn_df)
```

### Training & Testing Split

It's time to split our data into our training and testing sets. To be safe, we're going to stratify our randomly sampled data sets by `Churn` even though it's not terribly imbalanced.

```{r}
set.seed(1234)
churn_split <- initial_split(churn_df, strata = Churn)
churn_train <- training(churn_split)
churn_test <- testing(churn_split)
```

### 10-Fold Cross Validation

We also set up our cross-validation sets with the same stratification.

```{r}
set.seed(567)
churn_folds <- vfold_cv(churn_train, strata = Churn)
```

## Step 2 - First Model Specification

### Logistic Regression - Lasso

Our first model we'll try is a lasso model with a logistic regression from the `parsnip` package. Notice we set our penalty to `tune()`, we'll utilize our `churn_folds` to find a balance between the best numerically performing model a simpler model with less predictors.

```{r}
lasso_glm <- 
     parsnip::logistic_reg(
        penalty = tune(),
        mixture = 1
    ) %>% 
    set_engine("glmnet")
```

## Step 3 - Data Pre-Processing w/ Recipes

As mentioned previously, the bulk of our preprocessing steps are handled using a `recipe`

-   `update_role()` `customerID` column is only for ID purposes and will not be included
-   `step_dummy()` Convert all of our factor columns to binary
-   `step_normalize()` Center and scale all numeric columns
-   `step_zv()` Removes any variables that contain a single value

```{r}
churn_recipe <-
    recipe(Churn ~ ., data = churn_train) %>% 
    update_role(customerID, new_role = 'id') %>% 
    step_dummy(all_nominal_predictors()) %>% 
    step_normalize(all_numeric_predictors()) %>% 
    step_zv(all_predictors())

churn_recipe
```

## Step 4 - Tune Model Parameters

We start our lasso regularization with `tune_grid()` by tuning the penalty parameter. As the penalty is set higher - the model will remove under preforming predictor variables.

```{r}
lasso_workflow <- workflow(churn_recipe, lasso_glm)

doParallel::registerDoParallel()

set.seed(789)
lasso_grid <- tune_grid(
    lasso_workflow,
    resamples = churn_folds,
    grid = grid_regular(penalty(), levels = 100)
)
```

## Step 5 - Evaluate Results

Our `lasso_grid` is now tuned and ready for further analysis. Using `roc_auc` as our grading metric, we can visualize which models performed similarly during training even with a higher penalty value.

```{r}
lasso_grid %>% 
    collect_metrics() %>% 
    ggplot(aes(penalty, mean, color = .metric)) +
    geom_point(color = "black") +
    geom_line(size = 1.5, alpha = 0.5) +
    facet_wrap(~.metric, scales = "free", nrow = 2) +
    scale_x_log10(labels = scales::label_number()) +
    theme(legend.position = "bottom") +
    scale_fill_brewer(palette = "Set3") +
    theme_dark() 
```


### Choosing From our Candidate Models

Next we utilize `select_by_one_std_err()` to choose from our grid of potential model parameters. This function allows us to choose a simpler model (if available) if its performance is within one standard error of a more complicated yet higher performing model.

As you can see in the visual above, this analysis proves that a ***MUCH*** simpler model can be used to achieve a similar result.

```{r}
best_lasso <- select_by_one_std_err(
    lasso_grid, 
    metric = "roc_auc", 
    desc(penalty)
)

best_lasso
```

## Step 6 - Final Fit

It's time to finalize our model with our hold-out data, the metrics displayed below are our final results for this model.

```{r}
lasso_final_fit <- lasso_workflow %>% 
    finalize_workflow(best_lasso) %>% 
    last_fit(churn_split)

lasso_final_fit %>% 
    collect_metrics()
```

## Step 7 - Second Model Specification

### XGBoost

Now let's move on to our next model specification `xgboost`, notice we are tuning 3 different parameters with this powerful tree based model.

```{r}
xgb_spec <-
    boost_tree(
        trees = tune(),
        min_n = tune(),
        mtry = tune(),
        learn_rate = 0.01
    ) %>% 
    set_engine('xgboost') %>% 
    set_mode('classification')

xgb_workflow <- workflow(churn_recipe, xgb_spec)
```

## Step 8 - Fine Tune Model Params

[`tune_race_anova()`](https://finetune.tidymodels.org/reference/tune_race_anova.html) from the `finetune` package will eliminate combinations of different parameters that are not performing well using an `ANOVA` model. These methods are a fast an efficient way to handle grid search.

```{r}
doParallel::registerDoParallel()

set.seed(891)

library(finetune)

xgb_rs <- tune_race_anova(
    xgb_workflow,
    resamples = churn_folds,
    grid = 15,
    metrics = metric_set(roc_auc),
    control = control_race(verbose_elim = T)
)

xgb_rs
```

### Evaluate Results

`collect_metrics()` allows us to review the results of the grid search

```{r}
collect_metrics(xgb_rs)
```

### Visualize the Results 

We can also visualize this with `plot_race()`

```{r}
plot_race(xgb_rs)
```

## Step 9 - Choose and Finalize Fit

We select our best numerically preforming model based on `roc_auc` and finalize our model workflow. The metrics displayed are the final metrics obtained from our testing set.

```{r}
xgb_final_fit <- xgb_workflow %>% 
    finalize_workflow(select_best(xgb_rs, "roc_auc")) %>% 
    last_fit(churn_split)

collect_metrics(xgb_final_fit)
```

## Step 10 - Evaluate Final Preformance

Let's see how our model preformed with a confusion matrix.

```{r}
collect_predictions(xgb_final_fit) %>% 
    conf_mat(Churn, .pred_class) %>% 
    autoplot()
```

### Variable Importance

Using the package `vip` we can also gain valuable insight into which variables were most important to our model. It's no surprise that around 10 predictors had little to no significance in model performance after arriving at a high penalty value in our lasso model.

```{r}
library(vip)

xgb_final_fit %>% 
    extract_fit_engine() %>% 
    vip(20) 
```

### Compare Both Models

Let's compare our two models performance side by side

```{r}
xgb_final_fit %>%
  collect_predictions() %>% 
  roc_curve(Churn, .pred_No) %>% 
    mutate(.model = "XGBOOST") %>% 
    bind_rows(
        lasso_final_fit %>% 
            collect_predictions() %>% 
            roc_curve(Churn, .pred_No) %>% 
            mutate(.model = "LASSO")
    ) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = .model)) +
  geom_abline(slope = 1, color = "gray50", lty = 2, alpha = 0.8) +
  geom_path(size = 1.5, alpha = 0.7) +
  labs(color = NULL) +
  coord_equal()
```

## Notes

Finally we'll save these models to be used later in our Shiny application.

```{r}
readr::write_rds(extract_workflow(xgb_final_fit), 'app/trained_models/xgboost_racing_tuned.rds')
readr::write_rds(extract_workflow(lasso_final_fit), 'app/trained_models/lasso_penalty_tuned.rds')
```
